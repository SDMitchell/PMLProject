---
title: "Human Activity Recognition Prediction"
author: "SDMitchell"
date: "May 16, 2016"
output: html_document
---
  
```{r initial Variables, echo=FALSE}
randomSeed <- 13
set.seed(randomSeed)

machineLearningBaseURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn"
```

```{r Libraries, warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
library(knitr)
library(ggplot2)
library(tools)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
library(doParallel)
library(plyr)
library(ROCR)
#options(digits=4, scipen=0)
```
  
```{r Data Acquisition, echo=FALSE}
acquireData <- function(dataSetName) {
	# Try to download the given file if our input file does not already exist
	cacheFilename <- paste(dataSetName, "rds", sep=".")
	rawDataFilename <- paste(dataSetName, "csv", sep=".")
	if(!file.exists(cacheFilename))
	{
		# If the input data file doesn't exist yet, we need to download it
		if(!file.exists(rawDataFilename))
		{
			targetURL <- paste(machineLearningBaseURL, rawDataFilename, sep="/")
			download.file(url=targetURL, destfile=rawDataFilename, method="auto", mode="wb")
		}
	
		if(file.exists(rawDataFilename))
		{
			# Record the downloaded file's MD5 sum; this file's creation date can serve as the downloaded date
			write(md5sum(rawDataFilename), paste(rawDataFilename, ".MD5", sep=""))
		}
		else
		{
			stop("There was a problem attempting to download the file from the given URL")
		}
		# We either have the raw data file or it was already on disk. So here we are going to create an abbreviated data set
		# with just the features we want (no response, no new_window, no summary columns).
		dataRaw <- read.csv(rawDataFilename, header=TRUE, sep=",", stringsAsFactors=FALSE, na.strings="NA")
		dataSelection <- dataRaw[dataRaw$new_window == "no", grep("^var_|^avg|^max|^min|^amplitude|^stddev|^kurtosis|^skewness|new_window|num_window|.*?_timestamp|X|user_name|problem_id", colnames(dataRaw), invert=TRUE)]
		if("classe" %in% colnames(dataSelection))
			dataSelection$classe <- as.factor(dataSelection$classe)
		saveRDS(dataSelection, cacheFilename)
	}
	else
	{
		dataSelection <- readRDS(cacheFilename)
	}

	dataSelection
}
```

## Synopsis
We are performing an analysis of Human Activity data collected while participants were exercising both correctly and incorrectly. Using the data collected, we are attempting to predict if the subjects were indeed performing the activities correctly.

## Exploration
The data comes pre-divided into training and test sets, but since the training set is large enough, we are going to further divide the training data into a training and calibration set and leave the test set untouched until the final evaluation.
  
Simply looking at the training data, we can see that there are rows of summary statistics that look like samples but are in actuality completely dependent upon other samples. They seem to have been denoted by setting the *new_window* feature to the value "yes" so we can safely leave out these rows plus the *new_window* feature itself.
  
The summary statistics are also captured in their own set of features which we can safely ignore; they seem to be any feature starting with any of the following prefixes:
  
*"max_"*  
*"min_"*  
*"amplitude_"*  
*"var_"*  
*"stddev_"*  
*"kurtosis_"*  
*"skewness_"*  
  
The *classe* column denotes the value we are actually trying to predict, so we'll be using it in our models as the response. It seems to consist of a single capital letter in the range of A through E.
  
Several other statistics are of limited value to prediction algorithms (user name, trial number and timestamps for example) and they are also ignored in the analysis. It would appear that all other features are potentially usable after this cursory exploration.
  
***
  
## Data Preparation
To save a bit of time, we are going to construct our test set and save it into an R session with only the relevant samples and features. We also want to divide this set up into training and calibration sets; note that we set the random number generation seed at the beginning of the report.
```{r Data Preparation}
trainingdata <- acquireData("pml-training")
trainingdata$calibration <- runif(1:nrow(trainingdata)) > 0.70
realTrainingData <- subset(trainingdata[!trainingdata$calibration,], select=-c(calibration))
realCalibrationData <- subset(trainingdata[trainingdata$calibration,], select=-c(calibration))
realTestingData <- acquireData("pml-testing")
```

Since we are going to do 10-fold cross validation, we need a way to partition the data. This is simply an easy way; assign each row a random number between 1 and 10, then we can later use the for loop index to select the data.
```{r Fold generation}
# Generate a vector to be used for 10-fold cross-validation. It simply consists of nrow(realTrainingData) integers from 1-10
# that can be used in a simple for loop
foldNumber <- as.integer(runif(1:nrow(realTrainingData)) * 10) + 1

# We are selecting 1/5 of the data overall and then performing 10-fold CV. This is so that the random forest algo doesn't 
# take forever to complete and use up all of the system resources
inSampAccuracy <- replicate(10, 0)
outSampAccuracy <- replicate(10, 0)
```
  
  
  Investigate by counting (make sure there are no unique/rare occurences)
  
  Clustering for investigation? Makes a nice plot
  
  Support vector machine
  Used all of the data because it didn't take too much time
  The cross parameter is just for model parameter optimization/selection, not actual CV
```{r stuff}
#svmClasse <- svm(classe~., data=realTrainingData)
#predSVM <- predict(svmClasse, realCalibrationData)
#table(predSVM)
#count(realCalibrationData$classe == predSVM)
#predict(svmClasse, realTestingData)
dataWithFoldingInfo <- cbind(realTrainingData, foldNumber)

for(currentFold in 1:10)
{
	print(paste("Starting fold", currentFold))
	currentFoldTest <- dataWithFoldingInfo[dataWithFoldingInfo$foldNumber==currentFold, ]
	currentFoldTrain <- dataWithFoldingInfo[dataWithFoldingInfo$foldNumber!=currentFold, ]
	svmFit <- svm(classe~., data=subset(currentFoldTrain, select=-c(foldNumber)))
	svmPredTrain <- predict(svmFit, newdata=subset(currentFoldTrain, select=-c(foldNumber)))
	svmPredTest <- predict(svmFit, newdata=subset(currentFoldTest, select=-c(foldNumber)))
	inSampAccuracy[currentFold] <- count(currentFoldTrain$classe == svmPredTrain)[[2]][2] / length(svmPredTrain)
	outSampAccuracy[currentFold] <- count(currentFoldTest$classe == svmPredTest)[[2]][2] / length(svmPredTest)
}
mean(unlist(inSampAccuracy))
mean(unlist(outSampAccuracy))

```


1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
B  A  B  A  A  E  D  B  A  A  A  C  B  A  E  E  A  B  B  B 

Number 11 is wrong (correct answer is B)


Random Forest


Trying to run it with all of the data took an hour and generated a very poor model
Running with 1/10 of the data (randomly selected) generated a very good model and took a lot less time
```{r morestuff}
computationCluster <- makeCluster(8)
registerDoParallel(computationCluster)
dataWithFoldingInfo <- cbind(realTrainingData, foldNumber)[runif(1:nrow(realTrainingData)) > 0.80,]
#rfFit <- train(classe ~ ., data=realTrainingData, method="rf", prox=TRUE)

for(currentFold in 1:10)
{
	print(paste("Starting fold", currentFold))
	currentFoldTest <- dataWithFoldingInfo[dataWithFoldingInfo$foldNumber==currentFold, ]
	currentFoldTrain <- dataWithFoldingInfo[dataWithFoldingInfo$foldNumber!=currentFold, ]
	rfFit <- train(classe ~ ., data=subset(currentFoldTrain, select=-c(foldNumber)), method="rf", importance=TRUE)
	rfPredTrain <- predict(rfFit, newdata=subset(currentFoldTrain, select=-c(foldNumber)))
	rfPredTest <- predict(rfFit, newdata=subset(currentFoldTest, select=-c(foldNumber)))
	inSampAccuracy[currentFold] <- count(currentFoldTrain$classe == rfPredTrain)[[2]][2] / length(rfPredTrain)
	outSampAccuracy[currentFold] <- count(currentFoldTest$classe == rfPredTest)[[2]][2] / length(rfPredTest)
}
stopCluster(computationCluster)

mean(unlist(inSampAccuracy))
mean(unlist(outSampAccuracy))

```




```{r morestufftoo}
data <- as.data.frame(cbind(rownames(rfFit2$finalModel$importance),as.data.frame(importance(rfFit2$finalModel)[,"MeanDecreaseAccuracy"])))
colnames(data) <- c("Parameters","MeanDecreaseAccuracy")
data$MeanDecreaseAccuracy <- as.numeric(as.character(data$MeanDecreaseAccuracy))
(p <- ggplot(data) + geom_point(aes(MeanDecreaseAccuracy,Parameters)))
```


The RF managed to get #11 correct and #8 wrong	
B A B A A E D D A A B C B A E E A B B B
***
  
## Appendix A - About the Analysis
  
### Metadata
File Characteristic | Value
----------------- | -------
File Name | pml-training.csv
URL | [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
File Size | 12,202,745 bytes
Compression | None
MD5 | 56926c78af383dcdc2060407942e52e9
 | 
File Name | pml-testing.csv
URL | [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
File Size | 15,113 bytes
Compression | None
MD5 | bc4174f3ec5dfcc5c570a1d2709272d9

### Environment
**System Information**
```{r}
sysinf <- Sys.info()
```
Parameter | Value
-------- | --------
Operating System | `r paste(sysinf["sysname"], sysinf["release"], sep="")`
version | `r sysinf["version"]`
machine arch | `r sysinf["machine"]`

**Session Information**
```{r}
sessionInfo()
```
  
## References
* [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) - The original study where the data was collected
* [R Graphics Cookbook](http://shop.oreilly.com/product/0636920023135.do) - Winston Chang
* [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) - Gareth James, et al
* [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) - Trevor Hastie, Robert Tibshirani, Jerome Friedman
* [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) - Max Kuhn, Kjell Johnson
* [Practical Data Science With R](https://www.manning.com/books/practical-data-science-with-r) - Nina Zumel, John Mount

  
